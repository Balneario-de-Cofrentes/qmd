# SFT Training Config for QMD Query Expansion (Apple Silicon)
# Matches tobi's CUDA config as closely as possible

model:
  # Qwen3-1.7B - same as tobi's original
  base: "Qwen/Qwen3-1.7B"
  output: "qmd-query-expansion-1.7B-sft"

dataset:
  name: "tobil/qmd-query-expansion-train-v2"
  text_field: "text"
  eval_split: 0.1

training:
  batch_size: 4
  iters: 3000          # Match tobi's ~5 epochs
  learning_rate: 2e-4  # Match tobi's learning rate
  max_length: 512
  grad_accumulation_steps: 4

lora:
  num_layers: 16       # Number of layers to apply LoRA
  rank: 16             # LoRA rank
